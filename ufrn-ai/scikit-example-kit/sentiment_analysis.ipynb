{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "1e87f45ea6456f954092e3c66abf7b40f0e1d2f9e9713a423f4cac954e0c2624"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8OZSjc4WW9F"
      },
      "source": [
        "# Libraries import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJM_4FNfT5Ar"
      },
      "source": [
        "# Data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import multiprocessing\n",
        "\n",
        "# Fetch and label data\n",
        "!pip install whatthelang &> /dev/null\n",
        "!pip install vaderSentiment &> /dev/null\n",
        "!pip install afinn &> /dev/null\n",
        "!pip install twint &> /dev/null\n",
        "!pip install aiohttp==3.7.0 &> /dev/null\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from afinn import Afinn \n",
        "import twint\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Data cleaning\n",
        "from whatthelang import WhatTheLang\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Feature extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "# Feature scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import GridSearchCV, \\\n",
        "                                    train_test_split\n",
        "from sklearn.pipeline import Pipeline \n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, \\\n",
        "                            confusion_matrix, \\\n",
        "                            classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data fetching"
      ],
      "metadata": {
        "id": "w6m--CpKCKdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The opinion lexicon is a work of [Bing Liu](https://www.cs.uic.edu/~liub/) in the area of sentiment analysis and opinion mining from social media that assembles words classified as `negative` and `positive`, and much more.\n",
        "\n",
        "The focus of this work is to use both the positive/negative words to address a search with the Twitter API for Tweets that contains this words, therefore are most probable to address a positive/negative feeling.\n",
        "\n",
        "Since only using a lexicon is not enough to classify an entire sentence as expressing such feeling, other tools will be used to ensure such estimation for the Twitter extracted dataset labels."
      ],
      "metadata": {
        "id": "9OuBhhQ1AtoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n",
        "!wget -N http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar &> /dev/null\n",
        "!unrar e opinion-lexicon-English.rar -y &> /dev/null\n",
        "\n",
        "negativeWords = pd.read_csv('negative-words.txt', skiprows=30, encoding='ISO8859', names=['negative_words'], squeeze=True)\n",
        "positiveWords = pd.read_csv('positive-words.txt', skiprows=30, encoding='ISO8859', names=['positive_words'], squeeze=True)\n",
        "\n",
        "print('Opinion lexicon samples:\\n\\n' + \n",
        "      'Negative words:\\n' + \n",
        "      str(negativeWords.sample(5)) +'\\n\\n'+ \n",
        "      'Positive words:\\n' + \n",
        "      str(positiveWords.sample(5)) +'\\n')\n",
        "print('#negative_words = ' + str(len(negativeWords)))\n",
        "print('#positive_words = ' + str(len(positiveWords)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ATXXzCS5VFy",
        "outputId": "bf39bfb8-8076-4a19-af3e-6dbf5f0b6b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opinion lexicon samples:\n",
            "\n",
            "Negative words:\n",
            "3252    perfunctory\n",
            "4616          venom\n",
            "3007          na√Øve\n",
            "1812          fussy\n",
            "4767           wrip\n",
            "Name: negative_words, dtype: object\n",
            "\n",
            "Positive words:\n",
            "1516    reverently\n",
            "370        courage\n",
            "1974          wise\n",
            "623     excellence\n",
            "1149     masterful\n",
            "Name: positive_words, dtype: object\n",
            "\n",
            "#negative_words = 4783\n",
            "#positive_words = 2006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many tools for fetch data from Twitter, those are:\n",
        "- [TwitterSearch](https://twittersearch.readthedocs.io/);\n",
        "- [Tweepy](https://docs.tweepy.org/);\n",
        "- [TWINT](https://github.com/twintproject/twint);\n",
        "\n",
        "`TwitterSearch` share similarities with `Tweepy`, but both need to use the Twitter API. `TWINT` extracts the data in a different manner with a workaround to not use the Twitter API. \n",
        "\n",
        "This work will use the `TWINT` for the ease of use that comes no authentication requirements. So below is shown the process of importing the Tweets based in defined keywords as a demosntration. After this test the batch of tweets will e retrieved. \n",
        "\n",
        "`TWINT` allows to search on multiple keywords at a time just concatenating the string as `\"health OR juice\"` the tweets scraped will have `health` or `juice` or both keyword at the time. Using the opinion lexicon this could be achieved using the concatenation of the entire dataset as:\n",
        "\n",
        "```\n",
        "negativeWords.str.cat(sep=' OR ')\n",
        "positiveWords.str.cat(sep=' OR ')\n",
        "```\n",
        "\n",
        "But this would crash due to the size of the string. So the strategy of choice is to iterate over each word in the lexicon and scrape a fixed number of tweets based in each word.\n",
        "\n",
        "For this work each of the `6789` words in the lexicon will return `20` tweets, resulting in a total of `135780`. Further data cleaning will reduce this number."
      ],
      "metadata": {
        "id": "ylcQ9XNAzbgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = twint.Config()\n",
        "config.Pandas = True # use pandas integration\n",
        "config.Search = 'depression' # keyword to search\n",
        "config.Lang = 'en' # language\n",
        "config.Limit = 20 # config max. number of tweets returned (min. 20)\n",
        "config.Since = '2022-01-21 00:00:00' # fetch since data\n",
        "config.Until = '2022-01-28 00:00:00' # fetch until data\n",
        "#config.Hide_output = True # hide output of tweets when running\n",
        "#config.Store_csv = True # allow saving to csv\n",
        "#config.Output = \"custom_out.csv\" # save to filename\n",
        "twint.run.Search(config) "
      ],
      "metadata": {
        "id": "Ve59LmVtcsGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dac8ce4-41c9-4dd8-a64e-fd6c1aba02e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1486851509358940168 2022-01-27 23:59:58 +0000 <93418> @DVPdirect It's like people minimizing depression with nonsense like \"Have you tried just being happy?\" and \"You've got great things going on, you shouldn't be sad.\"\n",
            "1486851498520858627 2022-01-27 23:59:56 +0000 <Richard53116745> @dahuggasystem @elonmusk And cancer, depression etc etc. With Covid or from Covid, what age? Shove it, we are done with this nonsense.\n",
            "1486851489423413253 2022-01-27 23:59:54 +0000 <cillic> @solarpunkgirl As someone who has fought this feeling going on 38 years now, I need you to know that this is your depression LYING TO YOU.   It will do anything to win. You just have to recognize the lies for what they are.\n",
            "1486851484260179971 2022-01-27 23:59:52 +0000 <YesutorQ> @El_SahdGustavo @Mhiestroburns ....ùòÅùóµùó≤ùóª ùó∂ ùó∞ùóÆùóª ùòÄùó≤ùó≤ ùóµùóºùòÑ ùó±ùòÇùó∫ùóΩ ùòÅùóºùòÇ ùóπùóºùóºùó∏ ùóøùòÜùòÅ ùóªùóºùòÑ..Kep up the ignorance and pass it to your generation okay üòÉüòÉüòÉ 3b3 bua wo paaa.... prove point aa ,…îsi awaiting judgement..   Stop exhibiting Symptoms of hunger and depression...ü§£ü§£ü§£üò≠üò≠üò≠üò≠üò≠üò≠\n",
            "1486851484176334849 2022-01-27 23:59:52 +0000 <flawless_mydear> J‚Äôai un super job, je fais des √©tudes que j‚Äôaime mais non. Mon cerveau il me dit fuck et il m‚Äôenvoie la d√©pression. üòí\n",
            "1486851471589191683 2022-01-27 23:59:49 +0000 <ClouardjuanMado> @YMDTC Au vue des tweets üòÇüòÇüòÇ elle va faire une d√©pression a non c'est d√©j√† fait üòÇüòÇüòÇ l√† elle doit hurler sur une maquilleuse en coulisse\n",
            "1486851460877139969 2022-01-27 23:59:47 +0000 <AileenPelecio> @ceerearl Tapos pag binash sasabihin respect at may depression\n",
            "1486851448579280901 2022-01-27 23:59:44 +0000 <imissyoucali> with that being said, i've talked and talked about killing myself and wanting to die (depression), but never attempted..\n",
            "1486851402982907911 2022-01-27 23:59:33 +0000 <LuminescentCryo> @LiyueConsultant still refusing to cheer up due to Aether's words having a major impact on her mental health.*  I'm a burden....Zhongli. I was ALWAYS a fucking burden.  Wasn't that obvious?  *Now she was just sobbing so much unable to control her depression, despite her being in the arms of the+\n",
            "1486851382728699907 2022-01-27 23:59:28 +0000 <KyubiCrasher> Vent time:it's actually gotta really bad lately I mean obviously I'm doing ok and been having fun but.I just hate having depression so much I sit around all day then finally get to recording really late in the day.(Why uploads are so late) it just sucks hate being born like this\n",
            "1486851360167505926 2022-01-27 23:59:23 +0000 <ByronMan7901> @Gerry48245639 Does it come with a side of heartburn too? Perhaps a barf bag? OH, I KNOW, a side of depression and guilt! ü•¥üòµüòµ\n",
            "1486851353930522626 2022-01-27 23:59:21 +0000 <MsArianneMarie> My Dad passed away today. I can't feel anything.  I'm not sure what's wrong with me. I'm sad and bummed out 80% of the time and cry at commercials thanks to depression. My favorite and best guy dies in his room and I don't cry? I'm defective.\n",
            "1486851327279915010 2022-01-27 23:59:15 +0000 <StarTheTankist> @MarxHohenheim @ChaosIsMel if you talk just economically. They have a  ghost economy, so many business that should be bankrupt &amp; closed. Also there economy is struggling due to lack of immigration, because of racist attitudes. Also the fact company life has led to mass depression effecting economics\n",
            "1486851234636173319 2022-01-27 23:58:53 +0000 <thatpakishawty> I love having friends who are doctors because they can clinically diagnosed me with depression instead of just saying you‚Äôre having a bad week\n",
            "1486851226654461954 2022-01-27 23:58:51 +0000 <racheleditullio> @annaecook It doesn‚Äôt fix depression either.\n",
            "1486851216747507716 2022-01-27 23:58:49 +0000 <HeadphcneActor> Can someone smack me out of this depression pleas\n",
            "1486851203350900740 2022-01-27 23:58:45 +0000 <BrdsLikeJulio> @lukahndrxx Das treibt mich halt rr in die Depression 2018 war er sogar angek√ºndigt f√ºr ein Festival hier und dann kam irgendwie 2 Tage davor die Nachricht dass das doch nicht klappt  Ich kann rr nicht sterben ohne ihn live gesehen zu haben\n",
            "1486851144315912193 2022-01-27 23:58:31 +0000 <thejedijunkyard> the antidepressants are working for the depression but i still can‚Äôt focus for shit\n",
            "1486851102893129732 2022-01-27 23:58:21 +0000 <billy_s_nyc> If you‚Äôll excuse me, my back pain, seasonal depression and I are all about to climb in bed together.\n",
            "1486851080986185731 2022-01-27 23:58:16 +0000 <Viridian4892> Id like to give a special shoutout to seasonal depression. You‚Äôre a real mvp and true boss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to see that the line of code that ensures the output to a `.csv` file is commented. If wish so, this line can be uncommented and the `.csv` saved right away. But the intent of this works is only to save the Tweets column and no other information. All the columns are shown below:"
      ],
      "metadata": {
        "id": "wbVtA8RR0Fei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twint.output.panda.Tweets_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6FqB6tLzaZN",
        "outputId": "624402f2-35f4-4c04-d893-fdd509ff5dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'conversation_id', 'created_at', 'date', 'timezone', 'place',\n",
              "       'tweet', 'language', 'hashtags', 'cashtags', 'user_id', 'user_id_str',\n",
              "       'username', 'name', 'day', 'hour', 'link', 'urls', 'photos', 'video',\n",
              "       'thumbnail', 'retweet', 'nlikes', 'nreplies', 'nretweets', 'quote_url',\n",
              "       'search', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
              "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
              "       'trans_dest'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it can be filtered only the `tweet` column to a pandas dataframe and exported to `.csv`. Also the library `whatthelang` is used to remove tweets that are not in english."
      ],
      "metadata": {
        "id": "4KKp5xAw0fYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_lang(text):\n",
        "    try: \n",
        "        return wtl.predict_lang(text)\n",
        "    except Exception:\n",
        "        return 'exp'\n",
        "\n",
        "wtl = WhatTheLang()\n",
        "\n",
        "df = pd.DataFrame(twint.output.panda.Tweets_df['tweet'], columns=['tweet'])\n",
        "df['lang'] = df['tweet'].map(lambda t: detect_lang(t)) # detect language for each tweet\n",
        "df = df[df['lang'] == 'en'] # filter only english tweets\n",
        "df = df.drop(columns=['lang'])\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "_YvDrrs3zdBy",
        "outputId": "6196bf0b-1b00-4b36-ad79-d75f70a46810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2181acad-37e8-4efd-b192-0ac989aa65f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@DVPdirect It's like people minimizing depression with nonsense like \"Have you tried just being happy?\" and \"You've got great things going on, you shouldn't be sad.\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@dahuggasystem @elonmusk And cancer, depression etc etc. With Covid or from Covid, what age? Shove it, we are done with this nonsense.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@solarpunkgirl As someone who has fought this feeling going on 38 years now, I need you to know that this is your depression LYING TO YOU.   It will do anything to win. You just have to recognize the lies for what they are.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2181acad-37e8-4efd-b192-0ac989aa65f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2181acad-37e8-4efd-b192-0ac989aa65f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2181acad-37e8-4efd-b192-0ac989aa65f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                                                                                                                                             tweet\n",
              "0                                                            @DVPdirect It's like people minimizing depression with nonsense like \"Have you tried just being happy?\" and \"You've got great things going on, you shouldn't be sad.\"\n",
              "1                                                                                           @dahuggasystem @elonmusk And cancer, depression etc etc. With Covid or from Covid, what age? Shove it, we are done with this nonsense.\n",
              "2  @solarpunkgirl As someone who has fought this feeling going on 38 years now, I need you to know that this is your depression LYING TO YOU.   It will do anything to win. You just have to recognize the lies for what they are."
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then applying for the entirety  of the opinion lexicon:\n",
        "\n",
        "```{python}\n",
        "def detect_lang(text):\n",
        "    try: \n",
        "        return wtl.predict_lang(text)\n",
        "    except Exception:\n",
        "        return 'exp'\n",
        "def twint_config():\n",
        "  config = twint.Config()\n",
        "  config.Pandas = True # use pandas integration\n",
        "  config.Lang = 'en' # language\n",
        "  config.Limit = 20 # config max. number of tweets returned (min. 20)\n",
        "  config.Since = '2022-01-21 00:00:00' # fetch since data\n",
        "  config.Until = '2022-01-28 00:00:00' # fetch until data\n",
        "  config.Hide_output = True # hide output of tweets when running\n",
        "  return config\n",
        "\n",
        "keywords = pd.concat([positiveWords, negativeWords], axis=0).tolist() # keywords to iterate\n",
        "df = pd.DataFrame([], columns=['tweet']) # base dataframe to save data\n",
        "wtl = WhatTheLang() # what the language instance\n",
        "config = twint_config() # twint configuration \n",
        "missed_keywords = [] # list of keywords that return error\n",
        "n = len(keywords) # total number of keywords\n",
        "count = 0 # counting of progress\n",
        "save_ratio = 100 # save dataset in every #save_ratio scrapes\n",
        "\n",
        "for keyword in keywords:\n",
        "  try:\n",
        "    config.Search = keyword # keyword to search\n",
        "    twint.run.Search(config) # scape tweets with given keyword\n",
        "\n",
        "    temp = pd.DataFrame(twint.output.panda.Tweets_df['tweet'], columns=['tweet']) # get dataframe of scraped data\n",
        "    temp['lang'] = temp['tweet'].map(lambda t: detect_lang(t)) # detect language for each tweet\n",
        "    temp = temp[temp['lang'] == 'en'] # filter only english tweets\n",
        "    temp = temp.drop(columns=['lang']) # remove language column\n",
        "\n",
        "    df = pd.concat([df,temp], axis=0) # concatenate new tweets with previous batch\n",
        "    if((count%save_ratio)==(save_ratio-1)): \n",
        "      print(\"Progress: {:.0%}/100%\".format(count/n))\n",
        "      df.to_csv('dataset.csv')\n",
        "  except: \n",
        "    print('Error: the index is %d, and the missed keyword is %s' % (count, keywords[count]))\n",
        "    missed_keywords.append(keywords[count])\n",
        "  count +=1\n",
        "df.to_csv('dataset.csv'\n",
        "files.download('dataset.csv')\n",
        "```"
      ],
      "metadata": {
        "id": "5qUnOF6uCAL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet was executed in a host machine since Google Colab free quota does not support long time running. \n",
        "\n",
        "Some keywords did not returned tweets and other returned errors. Those were repeated two more times. Since the first run had a 1 week scope, the second run had 1 month scope and the last run had a 2 years scope. After this, the remaining keywords will not be repeated since after those 3 iterations there is enough to data to analyze."
      ],
      "metadata": {
        "id": "ssPBVM_8LKd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After generating the data in a host machine, the final step is to load the dataset that was stored in the GitHub. The non-english tweets removal with the `WhatTheLang` was not made in this case, so it should be done now."
      ],
      "metadata": {
        "id": "mYh_qS0bWxKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_lang(text):\n",
        "    try: \n",
        "        return wtl.predict_lang(text)\n",
        "    except Exception:\n",
        "        return 'exp'\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/Lwao/awesome-ai/main/ufrn-ai/datasets/tweets_dataset.csv', usecols=['tweet'])\n",
        "print('Length of dataset BEFORE removing non-english tweets: ' + str(len(df.index))) \n",
        "\n",
        "wtl = WhatTheLang()\n",
        "df['lang'] = df['tweet'].map(lambda t: detect_lang(t)) # detect language for each tweet\n",
        "df = df[df['lang'] == 'en'] # filter only english tweets\n",
        "df = df.drop(columns=['lang']) # drop language column\n",
        "df = df.reset_index()\n",
        "print('Length of dataset AFTER removing non-english tweets: ' + str(len(df.index)))\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBBvUhAMWwwZ",
        "outputId": "52d6519a-1562-429b-aab6-6b3f57a0d862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset BEFORE removing non-english tweets: 97869\n",
            "Length of dataset AFTER removing non-english tweets: 87043\n",
            "   index                                              tweet\n",
            "0      1  @tvtalker1 For a long ass time...  https://t.c...\n",
            "1      2  if you‚Äôve never sobbed in your car while liste...\n",
            "2      3  @frankamoctezuma @SenatorSoules If you had an ...\n",
            "3      4  when you actually have hair , you don‚Äôt always...\n",
            "4      5  @ShebayaSarai @SaunderHart @MaidQuitNoChef @Th...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Less than 10 thousand tweets were in a language different from english. The remaining dataset still large enough for good traning."
      ],
      "metadata": {
        "id": "QXLb8ltsZLm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternate data fetching"
      ],
      "metadata": {
        "id": "akWMZPYTx7Gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After initial tests, this works choose to base the data fetching in the keyword \"depression\" and its 43 synonyms trying to identify tweets related to mental health. The dataset is load below and removed tweets in non-english.\n",
        "\n",
        "The code used in the hsot machine is:\n",
        "\n",
        "```\n",
        "import twint\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def twint_config():\n",
        "  config = twint.Config()\n",
        "  config.Pandas = True # use pandas integration\n",
        "  config.Lang = 'en' # language\n",
        "  config.Limit = 100 # config max. number of tweets returned (min. 20)\n",
        "  config.Since = '2020-01-01 00:00:00' # fetch since data\n",
        "  config.Until = '2022-01-01 00:00:00' # fetch until data\n",
        "  \n",
        "  config.Hide_output = True # hide output of tweets when running\n",
        "  return config\n",
        "\n",
        "keywords = ['depression','abasement','abjection','blahs','bleakness','bummer','cheerlessness','dejection','desolation','desperation','despondency','discouragement','dispiritedness','distress','dole','dolefulness','dolor','downheartedness','dreariness','dullness','dumps','ennui','gloom','gloominess','heavyheartedness','hopelessness','lowness','melancholia','melancholy','misery','mortification','qualm','sadness','sorrow','trouble','unhappiness','vapors','woefulness','worry','abjectness','blue' 'funk','disconsolation','heaviness of heart','lugubriosity']\n",
        "config = twint_config() # twint configuration \n",
        "df = pd.DataFrame([], columns=['tweet']) # base dataframe to save data\n",
        "\n",
        "for keyword in keywords:\n",
        "  try:\n",
        "    config.Search = keyword # keyword to search\n",
        "    twint.run.Search(config) # scape tweets with given keyword\n",
        "    temp = pd.DataFrame(twint.output.panda.Tweets_df['tweet'], columns=['tweet'])\n",
        "    df = pd.concat([df,temp], axis=0) # concatenate new tweets with previous batch\n",
        "  except: df.to_csv('security_backup.csv')\n",
        "df.to_csv('tweet_depression_dataset_epanded.csv')\n",
        "```\n",
        "\n",
        "The data is taken from januray the 1th of 2020 to janury the 1th of 2022 parsing the period during the Covid-19 pandemic.\n",
        "\n",
        "Again the data was fetch in a host machine and then save in the web. Duplicates are removed prior to language detection."
      ],
      "metadata": {
        "id": "A9iuTtvMx9mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_lang(text):\n",
        "    try: \n",
        "        return wtl.predict_lang(text)\n",
        "    except Exception:\n",
        "        return 'exp'\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/Lwao/awesome-ai/main/ufrn-ai/datasets/tweets_drepression_dataset_expanded.csv', usecols=['tweet'])\n",
        "print('Length of dataset BEFORE removing non-english tweets: ' + str(len(df.index))) \n",
        "\n",
        "wtl = WhatTheLang()\n",
        "df['lang'] = df['tweet'].map(lambda t: detect_lang(t)) # detect language for each tweet\n",
        "df = df[df['lang'] == 'en'] # filter only english tweets\n",
        "df = df.drop(columns=['lang']) # drop language column\n",
        "df = df.reset_index()\n",
        "print('Length of dataset AFTER removing non-english tweets: ' + str(len(df.index)))\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1IKZUba0Bqb",
        "outputId": "8e0d5e2c-f95c-4c18-e3fc-a2b15c8cd8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset BEFORE removing non-english tweets: 3654\n",
            "Length of dataset AFTER removing non-english tweets: 3178\n",
            "   index                                              tweet\n",
            "0      0  @BarrennessBlack @Animalsdonthate @TwiceDonald...\n",
            "1      2     I have depression this is the best I√¢¬Ä¬ôve got.\n",
            "2      3  @cn0bles *sometimes* people who are hypersexua...\n",
            "3      4  I wish I had depression, but I'm falling and t...\n",
            "4      5     *depression cured fr*  https://t.co/9WB9KE8o24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('tweets_drepression_dataset_nonlabeled.csv')\n",
        "files.download('tweets_drepression_dataset_nonlabeled.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oBTORDzd02qz",
        "outputId": "9de76d86-1bb5-47f2-e9b7-5217d071d8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5f32d68f-342a-4b90-bf0f-2cad1b7e8ca6\", \"tweets_drepression_dataset_nonlabeled.csv\", 570734)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data labeling"
      ],
      "metadata": {
        "id": "1B2aDfJcCUf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the data is extracted based in the opinion lexicon, the following step is to apply data labeling to address meaningful and iformative sentiment analysis labels for each Tweet, thus providng context so that the machine learning models can learn from it. \n",
        "\n",
        "The tool of choice is the Valence Aware Dictionary and Sentiment Reasoner ([VADER](https://github.com/cjhutto/vaderSentiment)), a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. \n",
        "\n",
        "When provided a sentence VADER  is capable of addressing the rate of which represents a `positive`, `neutral` or `negative` sentiment in a scale that sums up to 1. There is an additional metric callend `compound` that varies from -1 up to 1 and i is basically an unidimensional measure of sentiment. Some useful thresholds of this metric are:\n",
        "\n",
        "1. positive sentiment: `compound` $\\geq 0.05$\n",
        "2. neutral sentiment: `compound` $> -0.05$ and `compound` $< 0.05$\n",
        "3. negative sentiment: `compound` $\\leq -0.05$\n",
        "\n",
        "The NLTK library also implements VADER, but this work choose to use the stand alone library for VADER.\n",
        "\n",
        "There are also others [sentiments lexicons](https://www.tidytextmining.com/sentiment.html#comparing-the-three-sentiment-dictionaries) that can be used to generate the classification labels and are based in single words (unigrams), such as:\n",
        "\n",
        "- [NRC](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) from Saif Mohammad and Peter Turney that categorizes words in a binary fashion likely a bag-of-words into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.\n",
        "- [BING](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) from Bing Liu and collaborators that categorizes words in a binary fashion into positive and negative categories, despite being simple, this was fitted the purpose to fetch the necessary data with the opinion lexicon;\n",
        "- [AFINN](https://github.com/fnielsen/afinn/blob/master/afinn/data/AFINN-en-165.txt) that assigns words with a score that between -5 (most negative sentiment) and 5 (most positive sentiment).\n",
        "\n",
        "In the context of this application both the NRC and BING are good choices to help data fetching and the BING was choose because its simplicity.\n",
        "\n",
        "Regarding data labeling, both AFINN and VADER are good choices. The choice for VADER was defined because its higher complexity and the fact that is attuned for social media feed.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9RiWcBtuClQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is shown an example using AFINN and since it computes the score based in the sum of each keyword weight, some undesirable situation can occur."
      ],
      "metadata": {
        "id": "smqyf82pJjbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [r\"I wish I can say I am happy\", r\"I've never been disappointed in my life\"]\n",
        "afinn = Afinn(language='en', emoticons=True)\n",
        "for sentence in sentences: print(\"'\"+sentence+\"' score: \"+ str(afinn.score(sentence)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRiO6Dh-KDHZ",
        "outputId": "2132c660-c3cb-404e-f9b8-9e7f66ec50b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'I wish I can say I am happy' score: 4.0\n",
            "'I've never been disappointed in my life' score: -2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first sentence `I wish I can say I am happy` should be classified as `negative`, but since it has the name \"happy\", this contributes to a `positive` classification. The same way the second sentence `I've never been disappointed in my life` was misclassified as `positive`"
      ],
      "metadata": {
        "id": "8LZ4HggvLdtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing the same sentences with VADER the result are shown below."
      ],
      "metadata": {
        "id": "vJuAhJdPNg_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [r\"I wish I can say I am happy\", r\"I've never been disappointed in my life\"]\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "for sentence in sentences: print(\"'\"+sentence+\"' score: \"+ str(analyzer.polarity_scores(sentence)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahuKJpcQMN16",
        "outputId": "f6033cca-38c6-4fb2-ea8a-d815aea9bd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'I wish I can say I am happy' score: {'neg': 0.0, 'neu': 0.484, 'pos': 0.516, 'compound': 0.7506}\n",
            "'I've never been disappointed in my life' score: {'neg': 0.0, 'neu': 0.701, 'pos': 0.299, 'compound': 0.3724}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even the first sentence being `negative`, the lexicon did not show any percentage of this sentiment, but the other metric such as `neutral`, `positive` and `compound` presents a degree of freedom to work with such sentences. The second sentence pperformed better, with no `negative` rate, but not entirely `positive`, thus preferind a `neutral` approach.\n",
        "\n",
        "Clearly those sentences are confusing and without a context they are useless. But the brief analysis have shown that VADER has more metrics, thus giving more degrees of freedom for labeling the dataset."
      ],
      "metadata": {
        "id": "t0DeL6juNorz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `compound` metric will address the labels for the dataset that will be used in this work. Since it varies from -1 to +1, i.e. from extreme negative to extreme positive, the approach is to multiply the `compound` metric by 10 and round for the nearest integer, so allowing to have 21 different classes of sentiment classification with the following sample space $\\text{label} = [-10,10] \\; \\forall l \\in \\mathbb{Z}$\n",
        "\n",
        "Applying the VADER sentiment lexicon for the in usage dataset and adjusting the label range, the following is obtained:"
      ],
      "metadata": {
        "id": "cAQB9pucZvSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "labels = []\n",
        "for sentence in df['tweet'].tolist(): \n",
        "  labels.append(np.round(analyzer.polarity_scores(sentence)['compound']*10))\n",
        "df['target'] = pd.Series(labels)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRBOli4CZ5WU",
        "outputId": "098dbdff-bcdc-4963-dcb2-13d8c3306e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 87043 entries, 0 to 87042\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   index   87043 non-null  int64  \n",
            " 1   tweet   87043 non-null  object \n",
            " 2   target  87043 non-null  float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 2.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step marks the end of the data labeling. This dataset is now ready for further cleaning, pre-processing and analysis. One more time, this milestone will be saved sothe next steps can progress from here and on."
      ],
      "metadata": {
        "id": "4noxbeSYc_Au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "0cWZKe_2dOpT",
        "outputId": "9158ddb2-828a-43cc-f6a4-9d24f71b0172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e08ca120-bd53-4c7e-884f-91f94f402742\", \"tweets_dataset_labeled.csv\", 16569620)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternate data labeling"
      ],
      "metadata": {
        "id": "ZKGjEVfVzZ4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section the alternate dataset is loaded for labeling and then saved."
      ],
      "metadata": {
        "id": "tm8Ow0jkzcdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Lwao/awesome-ai/main/ufrn-ai/datasets/tweets_drepression_dataset_nonlabeled.csv', usecols=['tweet'])\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "labels = []\n",
        "for sentence in df['tweet'].tolist(): \n",
        "  labels.append(np.round(analyzer.polarity_scores(sentence)['compound']*10))\n",
        "df['target'] = pd.Series(labels)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19FbuIHH0v7o",
        "outputId": "9a050fd8-9884-4c2d-8cef-51bad04226ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3178 entries, 0 to 3177\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   tweet   3178 non-null   object \n",
            " 1   target  3178 non-null   float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 49.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('tweets_drepression_dataset.csv')\n",
        "files.download('tweets_drepression_dataset.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5QvLuw5H0xQr",
        "outputId": "e7502f54-c0a4-4f92-a3d5-075aee640297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0b755698-db31-4b9b-a3c3-96754a167e02\", \"tweets_drepression_dataset.csv\", 570797)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning"
      ],
      "metadata": {
        "id": "JTQkgJT9d8o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the previous milestone, the dataset can be loaded."
      ],
      "metadata": {
        "id": "ZzmT0yOqeUVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Lwao/awesome-ai/main/ufrn-ai/datasets/tweets_drepression_dataset.csv', usecols=['tweet','target'])\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df.sample(10).head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "MHno1ECGd-t6",
        "outputId": "e468017b-51fe-4e36-a315-9df88495eb2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ae5bf68f-af38-4645-8be7-9466a26f5c69\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1387</th>\n",
              "      <td>Guess what time it is for me?  That's right, fatality sickening depression time. √É¬∞√Ç¬ü√Ç¬í√Ç¬î√É¬¢√Ç¬ò√Ç¬†√É¬Ø√Ç¬∏√Ç¬è</td>\n",
              "      <td>-9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2954</th>\n",
              "      <td>√¢¬Ä¬úYeah, we are arriving on 23rd so we can unpack, make ourselves at home, decorate everything√¢¬Ä¬¶√¢¬Ä¬ù √°¬í¬çk said pretending to be oblivious to the way √°¬í¬çm√¢¬Ä¬ôs eyes flickered with gloominess.</td>\n",
              "      <td>-3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1184</th>\n",
              "      <td>That day after Christmas sadness is reaaal</td>\n",
              "      <td>-4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2602</th>\n",
              "      <td>it's one step closer to 1995 tetsuya's hair. omg the day my hair is that length is the day my depression will disappear</td>\n",
              "      <td>-7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1357</th>\n",
              "      <td>the world loves letting me know that it does not work for me, but actually works as a well-paid professional against me #depression #anxiety #ugh           helpppppp .√É¬Ø√Ç¬Ω√Ç¬°√É¬Ø√Ç¬Ω√Ç¬•√É¬Ø√Ç¬æ√Ç¬ü√É¬Ø√Ç¬æ√Ç¬ü√É¬Ø√Ç¬Ω√Ç¬•(√É¬Ø√Ç¬Ω√Ç¬°√É¬¢√Ç¬Ä√Ç¬¢√É¬å√Ç¬Å√É¬Ø√Ç¬∏√Ç¬ø√É¬¢√Ç¬Ä√Ç¬¢√É¬å√Ç¬Ä√É¬Ø√Ç¬Ω√Ç¬°)√É¬Ø√Ç¬Ω√Ç¬•√É¬Ø√Ç¬æ√Ç¬ü√É¬Ø√Ç¬æ√Ç¬ü√É¬Ø√Ç¬Ω√Ç¬•√É¬Ø√Ç¬Ω√Ç¬°.</td>\n",
              "      <td>-9.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae5bf68f-af38-4645-8be7-9466a26f5c69')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae5bf68f-af38-4645-8be7-9466a26f5c69 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae5bf68f-af38-4645-8be7-9466a26f5c69');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                             tweet  target\n",
              "1387                                                                                                                                                                         Guess what time it is for me?  That's right, fatality sickening depression time. √É¬∞√Ç¬ü√Ç¬í√Ç¬î√É¬¢√Ç¬ò√Ç¬†√É¬Ø√Ç¬∏√Ç¬è    -9.0\n",
              "2954                                                                                 √¢¬Ä¬úYeah, we are arriving on 23rd so we can unpack, make ourselves at home, decorate everything√¢¬Ä¬¶√¢¬Ä¬ù √°¬í¬çk said pretending to be oblivious to the way √°¬í¬çm√¢¬Ä¬ôs eyes flickered with gloominess.    -3.0\n",
              "1184                                                                                                                                                                                                                                    That day after Christmas sadness is reaaal    -4.0\n",
              "2602                                                                                                                                                       it's one step closer to 1995 tetsuya's hair. omg the day my hair is that length is the day my depression will disappear    -7.0\n",
              "1357  the world loves letting me know that it does not work for me, but actually works as a well-paid professional against me #depression #anxiety #ugh           helpppppp .√É¬Ø√Ç¬Ω√Ç¬°√É¬Ø√Ç¬Ω√Ç¬•√É¬Ø√Ç¬æ√Ç¬ü√É¬Ø√Ç¬æ√Ç¬ü√É¬Ø√Ç¬Ω√Ç¬•(√É¬Ø√Ç¬Ω√Ç¬°√É¬¢√Ç¬Ä√Ç¬¢√É¬å√Ç¬Å√É¬Ø√Ç¬∏√Ç¬ø√É¬¢√Ç¬Ä√Ç¬¢√É¬å√Ç¬Ä√É¬Ø√Ç¬Ω√Ç¬°)√É¬Ø√Ç¬Ω√Ç¬•√É¬Ø√Ç¬æ√Ç¬ü√É¬Ø√Ç¬æ√Ç¬ü√É¬Ø√Ç¬Ω√Ç¬•√É¬Ø√Ç¬Ω√Ç¬°.    -9.0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next the NLTK library will be used for some cleaning in the textual data, such as:\n",
        "\n",
        "- Remove unwanted characters and links;\n",
        "- Remove stopwords that the sole purpose is to connect nouns and others meaningful words;\n",
        "- Stemming to reduce words to its radical, i.e. short format;\n",
        "- Lemmatization to reduce inflection words to a word that represents its motto;\n",
        "\n",
        "\n",
        "Some resources from NLTK must be downloaded before applying the data cleaning."
      ],
      "metadata": {
        "id": "81SPXeiil40C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhOENBdsxmo-",
        "outputId": "a5aea25f-f5d0-47f3-cf85-a8b5eea55a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below a test sentence extracted from the dataset is used to test if each cleaning process is indeed useful or otherwise spoil the data."
      ],
      "metadata": {
        "id": "8KFnobovvNHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://minerandodados.com.br/analise-de-sentimentos-utilizando-dados-do-twitter/\n",
        "\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "test_sentence = \"@DeWalle80 @kingbooh_ @BarcaNaija @onovoxvii Pls snare my questions https://t.co/QdipCF4MCY\"\n",
        "print('Original test sentence: ' + test_sentence)\n",
        "\n",
        "def RemoveUnwantedChars(instance): # remove links, dots, comas, etc.\n",
        "  instance = re.sub(r\"http\\S+\", \"\", instance).lower().replace('.','').replace(';','').replace('-','').replace(':','').replace(')','')\n",
        "  return (instance)\n",
        "test_sentence = RemoveUnwantedChars(test_sentence)\n",
        "print('Remove unwanted chars result: ' + test_sentence)\n",
        "\n",
        "def RemoveStopWords(instance, stopwords):\n",
        "  words = [i for i in instance.split() if not i in stopwords]\n",
        "  return (\" \".join(words))\n",
        "test_sentence = RemoveStopWords(test_sentence, stopwords)\n",
        "print('Remove stopwords result: ' + test_sentence)\n",
        "\n",
        "def Stemming(instance, stemmer):\n",
        "  words = []\n",
        "  for w in instance.split():\n",
        "      words.append(stemmer.stem(w))\n",
        "  return (\" \".join(words))\n",
        "test_sentence = Stemming(test_sentence, stemmer)\n",
        "print('Stemming result: ' + test_sentence)\n",
        "\n",
        "def Lemmatization(instance, lemmatizer):\n",
        "  words = []\n",
        "  for w in instance.split():\n",
        "    words.append(lemmatizer.lemmatize(w))\n",
        "  return (\" \".join(words))\n",
        "test_sentence = Lemmatization(test_sentence, lemmatizer)\n",
        "print('Lemmatization result: ' + test_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brKX8HoImzA5",
        "outputId": "ae536f1e-8a06-4327-eb1d-2be411bb59f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original test sentence: @DeWalle80 @kingbooh_ @BarcaNaija @onovoxvii Pls snare my questions https://t.co/QdipCF4MCY\n",
            "Remove unwanted chars result: @dewalle80 @kingbooh_ @barcanaija @onovoxvii pls snare my questions \n",
            "Remove stopwords result: @dewalle80 @kingbooh_ @barcanaija @onovoxvii pls snare questions\n",
            "Stemming result: @dewalle80 @kingbooh_ @barcanaij @onovoxvi pl sn questiom\n",
            "Lemmatization result: @dewalle80 @kingbooh_ @barcanaij @onovoxvi pl sn questiom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final step, all those modifiers are applied to the entirety of the dataset in the following order:\n",
        "\n",
        "- Remove unwanted characters;\n",
        "- Remove stopwords;\n",
        "- Stemming;\n",
        "- Lemmatization;"
      ],
      "metadata": {
        "id": "1pOOZcE_0JuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BatchPreprocessing(instance, stopwords=None, stemmer=None, lemmateizer=None):\n",
        "  instance = RemoveUnwantedChars(instance)\n",
        "  if(stopwords!=None): instance = RemoveStopWords(instance, stopwords)\n",
        "  if(stemmer!=None): instance = Stemming(instance, stemmer)\n",
        "  if(lemmatizer!=None): instance = Lemmatization(instance, lemmatizer)\n",
        "  return instance\n",
        "\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "preprocessed_tweets = [BatchPreprocessing(tweet, stopwords, stemmer, lemmatizer) for tweet in df['tweet']]\n",
        "df_preprocessed = df.copy()\n",
        "df_preprocessed['tweet'] = pd.Series(preprocessed_tweets)\n",
        "df_preprocessed.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-75JNqIdsL6M",
        "outputId": "cb91e58b-f171-40d7-f844-1706d9529cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b158068d-1dd9-40ab-a6b4-2d448ce70f9c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@barrennessblack @animalsdonthat @twicedonald @ksorb @twittersupport feel depression mil away i'm sorry hop find lov desperately need</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>depression best i√¢¬Ä¬ôv got</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@cn0bl *sometimes* peopl hypersex depression √¢¬Ä¬¶ that√¢¬Ä¬ô everyon tbh</td>\n",
              "      <td>-3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wish depression, i'm falling end</td>\n",
              "      <td>-6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>*depression cured fr*</td>\n",
              "      <td>-6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b158068d-1dd9-40ab-a6b4-2d448ce70f9c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b158068d-1dd9-40ab-a6b4-2d448ce70f9c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b158068d-1dd9-40ab-a6b4-2d448ce70f9c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                                                   tweet  target\n",
              "0  @barrennessblack @animalsdonthat @twicedonald @ksorb @twittersupport feel depression mil away i'm sorry hop find lov desperately need     1.0\n",
              "1                                                                                                              depression best i√¢¬Ä¬ôv got     1.0\n",
              "2                                                                   @cn0bl *sometimes* peopl hypersex depression √¢¬Ä¬¶ that√¢¬Ä¬ô everyon tbh    -3.0\n",
              "3                                                                                                       wish depression, i'm falling end    -6.0\n",
              "4                                                                                                                  *depression cured fr*    -6.0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('tweets_depression_dataset_preprocessed.csv')\n",
        "files.download('tweets_depression_dataset_preprocessed.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nbq2QKwTdv0z",
        "outputId": "5c5675b5-d211-49bb-ae08-8178f98cc980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_89bd35a8-5ecd-4956-a1fe-6b99ec3f690b\", \"tweets_depression_dataset_preprocessed.csv\", 570797)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extraction"
      ],
      "metadata": {
        "id": "fW1dJ9U2e-6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the previous milestone, the dataset can be loaded."
      ],
      "metadata": {
        "id": "CfKUdVcVeTsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Lwao/awesome-ai/main/ufrn-ai/datasets/tweets_depression_dataset_preprocessed.csv', usecols=['tweet','target'])\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df.sample(10).head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "I1JkZ1yqeQiR",
        "outputId": "6549aa00-b764-4206-f738-63d8228b16e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-037ee59e-1cf6-425c-b2e3-71adfd393ff1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>842</th>\n",
              "      <td>@shehackspurple Oh yes a No-Op or NOP. I have seen a few used in malware over the years. IPS usually detects them after a few iterations but I have also seen different NOPs strung together with obfuscation to avoid detection. It seems like good fun for programming</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915</th>\n",
              "      <td>@TylerFeldmanTV @Lin_Manuel just.....that man reeeeeally delights in threading the needle between 'delightful evocative art' and 'negligent infliction of emotional distress.'</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2328</th>\n",
              "      <td>@lorenxco14 @btrflyclips your acting like their whole entire family was murdered and they are experiencing horrible depression and self crippling anxiety from it. they will be fine, they'll live. they'll get over it.</td>\n",
              "      <td>-9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3083</th>\n",
              "      <td>good night, good night  parting is such sweet sorrow, that I shall say good night till it be morrow  &amp;lt;3</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1104</th>\n",
              "      <td>Where her medical team took any salvageable organs and declared her dead. She was 29. She had a 4 year old daughter. She battled addictions and homelessness for years.</td>\n",
              "      <td>-8.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-037ee59e-1cf6-425c-b2e3-71adfd393ff1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-037ee59e-1cf6-425c-b2e3-71adfd393ff1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-037ee59e-1cf6-425c-b2e3-71adfd393ff1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                         tweet  target\n",
              "842   @shehackspurple Oh yes a No-Op or NOP. I have seen a few used in malware over the years. IPS usually detects them after a few iterations but I have also seen different NOPs strung together with obfuscation to avoid detection. It seems like good fun for programming     9.0\n",
              "915                                                                                             @TylerFeldmanTV @Lin_Manuel just.....that man reeeeeally delights in threading the needle between 'delightful evocative art' and 'negligent infliction of emotional distress.'     6.0\n",
              "2328                                                  @lorenxco14 @btrflyclips your acting like their whole entire family was murdered and they are experiencing horrible depression and self crippling anxiety from it. they will be fine, they'll live. they'll get over it.    -9.0\n",
              "3083                                                                                                                                                                good night, good night  parting is such sweet sorrow, that I shall say good night till it be morrow  &lt;3     8.0\n",
              "1104                                                                                                   Where her medical team took any salvageable organs and declared her dead. She was 29. She had a 4 year old daughter. She battled addictions and homelessness for years.    -8.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This work will use multiple [text vectorization/feature extraction](https://towardsdatascience.com/getting-started-with-text-vectorization-2f2efbec6685) methods, they are:\n",
        "\n",
        "- Binary Term Frequency (One-Hot-Encoder);\n",
        "- Bag of Words (BoW) Term Frequency;\n",
        "- (L1) Normalized Term Frequency;\n",
        "- (L2) Normalized TF-IDF;\n",
        "- Word2Vec.\n",
        "\n",
        "The first four can be implemented using TF-IDF from `Sklearn` library with some adjusts and the last will be used with the help of the `Gensim` library.\n",
        "\n",
        "1. The One-Hot-Encoder is achieved seting the `TfidfVectorizer` binary parameter to true and norm to false;\n",
        "2. To obtain the Bag-of-Word the binary parameter in changed to false so it can show the term frequency and norm to none;\n",
        "3. To obtain the TF the binary parameter still false, but the norm changes to L1;\n",
        "4. To obtain the TF-IDF the binary parameter still fasle, but the norm is changed to L2 and the parameters of using IDF and smooth IDF are change to true."
      ],
      "metadata": {
        "id": "OV0FmXedfB_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_vectorizer(model='ohe'):\n",
        "  if(model=='ohe'):\n",
        "    tv = TfidfVectorizer(\n",
        "        binary=True, norm=False, use_idf=False, smooth_idf=False,\n",
        "        lowercase=True, stop_words='english', min_df=1, max_df=1.0,\n",
        "        ngram_range=(1,1), max_features=1000\n",
        "    )\n",
        "  elif(model=='bow'):\n",
        "    tv = TfidfVectorizer(\n",
        "        binary=False, norm=None, use_idf=False, smooth_idf=False,\n",
        "        lowercase=True, stop_words='english', min_df=1, max_df=1.0,\n",
        "        ngram_range=(1,1), max_features=1000\n",
        "    )\n",
        "  elif(model=='tf'):\n",
        "    tv = TfidfVectorizer(\n",
        "        binary=False, norm='l1', use_idf=False, smooth_idf=False,\n",
        "        lowercase=True, stop_words='english', min_df=1, max_df=1.0,\n",
        "        ngram_range=(1,1), max_features=1000\n",
        "    )\n",
        "  elif(model=='tf-idf'):\n",
        "    tv = TfidfVectorizer(\n",
        "        binary=False, norm='l2', use_idf=True, smooth_idf=True,\n",
        "        lowercase=True, stop_words='english', min_df=1, max_df=1.0,\n",
        "        ngram_range=(1,1), max_features=1000\n",
        "    )\n",
        "  return tv"
      ],
      "metadata": {
        "id": "ntx6d9s0QCVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering the `Word2Vec` implementation, it was used with vectors of size 1000 for each word in the dataset. Following the trainined `Word2Vec` model on top of the dataset vocabulary was iterated with every word en each sentence of the dataset. To get the vector representation of each sentence, it was chosen the strategy of getting the mean value of all vector of each word in the sentence."
      ],
      "metadata": {
        "id": "Hq6PwNg_TKMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Word2Vec (https://gist.github.com/giuseppebonaccorso/061fca8d0dfc6873619efd8f364bfe89, https://machinelearningmastery.com/develop-word-embeddings-python-gensim/)\n",
        "tokenized_data = [str_.split() for str_ in df['tweet'].tolist()] # tokenize data\n",
        "model = Word2Vec(sentences=tokenized_data, size=1000, min_count=1, workers=multiprocessing.cpu_count()) # train word2vec model\n",
        "print('Word2Vec model summary:')\n",
        "print(model)\n",
        "\n",
        "# apply mean value of vector in each word of a given sentence for all tweets\n",
        "m = len(tokenized_data)\n",
        "matrix_ = np.zeros((m,1000))\n",
        "for i in range(m):\n",
        "  n = len(tokenized_data[i][:])\n",
        "  for j in range(n):\n",
        "    matrix_[i,:] += (model.wv[tokenized_data[i][j]]/n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds9MHOOkFQE5",
        "outputId": "d3684868-a52d-45cf-c39e-32127f28a616"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec model summary:\n",
            "Word2Vec(vocab=20114, size=1000, alpha=0.025)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a little toy set of data all text vectorizers can be tested before applying to the entire dataset. `Word2Vec` was ot tested since its content depends on the data."
      ],
      "metadata": {
        "id": "qIbg9CumT5mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = ['dogs bark','cats meow','dogs bark, cats meow','dogs bark, cats dont bark','cats meow, dogs dont meow']\n",
        "print('Sample of One-Hot-Encoder:')\n",
        "tv = get_text_vectorizer(model='ohe')\n",
        "print(pd.DataFrame(tv.fit_transform(test_sentences).toarray(), columns=tv.get_feature_names_out()))\n",
        "print('Sample of Bag-of-Words:')\n",
        "tv = get_text_vectorizer(model='bow')\n",
        "print(pd.DataFrame(tv.fit_transform(test_sentences).toarray(), columns=tv.get_feature_names_out()))\n",
        "print('Sample of TF:')\n",
        "tv = get_text_vectorizer(model='tf')\n",
        "print(pd.DataFrame(tv.fit_transform(test_sentences).toarray(), columns=tv.get_feature_names_out()))\n",
        "print('Sample of TF-IDF:')\n",
        "tv = get_text_vectorizer(model='tf-idf')\n",
        "print(pd.DataFrame(tv.fit_transform(test_sentences).toarray(), columns=tv.get_feature_names_out()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uirse1X6UMHo",
        "outputId": "597b1ee0-48e4-4527-b0a5-b9d652b2a846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of One-Hot-Encoder:\n",
            "   bark  cats  dogs  dont  meow\n",
            "0   1.0   0.0   1.0   0.0   0.0\n",
            "1   0.0   1.0   0.0   0.0   1.0\n",
            "2   1.0   1.0   1.0   0.0   1.0\n",
            "3   1.0   1.0   1.0   1.0   0.0\n",
            "4   0.0   1.0   1.0   1.0   1.0\n",
            "Sample of Bag-of-Words:\n",
            "   bark  cats  dogs  dont  meow\n",
            "0   1.0   0.0   1.0   0.0   0.0\n",
            "1   0.0   1.0   0.0   0.0   1.0\n",
            "2   1.0   1.0   1.0   0.0   1.0\n",
            "3   2.0   1.0   1.0   1.0   0.0\n",
            "4   0.0   1.0   1.0   1.0   2.0\n",
            "Sample of TF:\n",
            "   bark  cats  dogs  dont  meow\n",
            "0  0.50  0.00  0.50   0.0  0.00\n",
            "1  0.00  0.50  0.00   0.0  0.50\n",
            "2  0.25  0.25  0.25   0.0  0.25\n",
            "3  0.40  0.20  0.20   0.2  0.00\n",
            "4  0.00  0.20  0.20   0.2  0.40\n",
            "Sample of TF-IDF:\n",
            "       bark      cats      dogs     dont      meow\n",
            "0  0.765241  0.000000  0.643744  0.00000  0.000000\n",
            "1  0.000000  0.643744  0.000000  0.00000  0.765241\n",
            "2  0.541107  0.455196  0.455196  0.00000  0.541107\n",
            "3  0.763236  0.321029  0.321029  0.45973  0.000000\n",
            "4  0.000000  0.321029  0.321029  0.45973  0.763236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is easy to spot the difference between each vectorizer.\n",
        "\n",
        "Each vectorizer will be applied to the dataset and the results will be stored in a dictionary of dataframes for further training."
      ],
      "metadata": {
        "id": "S9vvUWfxWl88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dict = {'ohe':pd.DataFrame(), 'bow':pd.DataFrame(), 'tf':pd.DataFrame(), 'tf-idf':pd.DataFrame(), 'word2vec':pd.DataFrame()}\n",
        "keys = list(df_dict.keys())\n",
        "for key in keys[:-1]:\n",
        "  tv = get_text_vectorizer(model=key)\n",
        "  df_dict[key] = pd.DataFrame(tv.fit_transform(df['tweet']).toarray())\n",
        "df_dict['word2vec'] = pd.DataFrame(matrix_)"
      ],
      "metadata": {
        "id": "hCAuyhgPZ_Es"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning training"
      ],
      "metadata": {
        "id": "zXXRQ2gMYeWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the final section once all machine learning models will be applied and the results analyzed.\n",
        "\n",
        "The tests will use 4 different ML models:\n",
        "\n",
        "- KNN;\n",
        "- Multinomial Naive Bayes ([this subset of naive Bayes is more adequate for textual data](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB));\n",
        "- Decision tree;\n",
        "- Random forest;\n",
        "\n",
        "Each model will run with a grid search in a default set of hyperparameters. The features will be scaled with a min-max scaler. Each model will run for each text vectorizer method and the different vectorizers will be compared inside the same ML model."
      ],
      "metadata": {
        "id": "1PmmLP3pYkoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(data_dict, y, keys, train_per=0.9, clf='knn'):\n",
        "  # Build search space for the model \n",
        "  steps = []\n",
        "  steps.append(('scaler', MinMaxScaler()))\n",
        "  \n",
        "  if(clf=='knn'):\n",
        "    steps.append(('clf', KNeighborsClassifier(algorithm='auto', n_jobs=-1, leaf_size=100)))\n",
        "    param_grid = {\n",
        "      'clf__n_neighbors': [1, 3, 5, 7],\n",
        "      'clf__weights': ['uniform', 'distance'], \n",
        "      'clf__metric': ['euclidean', 'minkowski']\n",
        "    }\n",
        "  elif(clf=='naive_bayes'):\n",
        "    steps.append(('clf', MultinomialNB()))\n",
        "    param_grid = {\n",
        "      'clf__alpha': [1e-3, 1e-2, 1e-1, 1e0]\n",
        "    }\n",
        "  elif(clf=='decision_tree'):\n",
        "    steps.append(('clf', DecisionTreeClassifier()))\n",
        "    param_grid = {\n",
        "      'clf__criterion': ['gini', 'entropy'],\n",
        "      'clf__max_depth': [None, 5, 10, 15],\n",
        "    }\n",
        "  elif(clf=='random_forest'): \n",
        "    steps.append(('clf', RandomForestClassifier()))  \n",
        "    param_grid = {\n",
        "      'clf__n_estimators': [10, 50, 100, 200, 300],\n",
        "      'clf__criterion': ['gini', 'entropy'],\n",
        "      'clf__max_depth': [None, 5, 10, 15]\n",
        "    }\n",
        "\n",
        "  y = y.ravel()\n",
        "  for key in keys:\n",
        "    X = data_dict[key]\n",
        "    # Holdout \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_per, random_state=42, shuffle=True)\n",
        "\n",
        "    # Grid search\n",
        "    start = time.process_time()\n",
        "    pipe = Pipeline(steps)\n",
        "    model = GridSearchCV(pipe, param_grid, n_jobs=-1, verbose=False)\n",
        "    model = model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print('Grid search duration: ' + str(time.process_time()-start) + ' seconds')\n",
        "\n",
        "    # Metrics\n",
        "    print('Model metrics:')\n",
        "    print('='*30)\n",
        "    print('Vectorizer: ' + key)\n",
        "    print('Best hyperparameters: ' + str(model.best_params_))\n",
        "    print('Best score: ' + str(model.score(X_test, y_test)))\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "EzPZpmIs5d6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering all vectorizer options:"
      ],
      "metadata": {
        "id": "_JFQiprYYwU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(keys)"
      ],
      "metadata": {
        "id": "2BRcZcC8FDE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b299c4bf-239e-4bbe-ba59-2f26caa109de"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ohe', 'bow', 'tf', 'tf-idf', 'word2vec']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training for KNN:"
      ],
      "metadata": {
        "id": "EYdOG6quYzv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run(df_dict, df['target'], keys, train_per=0.9, clf='knn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsWxNqqcmYzy",
        "outputId": "c8af3236-33cb-4dea-ba43-68177d09c7f8"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid search duration: 1.2705906329999834 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: ohe\n",
            "Best hyperparameters: {'clf__metric': 'euclidean', 'clf__n_neighbors': 3, 'clf__weights': 'distance'}\n",
            "Best score: 0.27672955974842767\n",
            "\n",
            "\n",
            "Grid search duration: 1.0486953299999868 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: bow\n",
            "Best hyperparameters: {'clf__metric': 'euclidean', 'clf__n_neighbors': 5, 'clf__weights': 'distance'}\n",
            "Best score: 0.2578616352201258\n",
            "\n",
            "\n",
            "Grid search duration: 1.1731510629999775 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: tf\n",
            "Best hyperparameters: {'clf__metric': 'euclidean', 'clf__n_neighbors': 1, 'clf__weights': 'uniform'}\n",
            "Best score: 0.18867924528301888\n",
            "\n",
            "\n",
            "Grid search duration: 1.0328542510000034 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: tf-idf\n",
            "Best hyperparameters: {'clf__metric': 'euclidean', 'clf__n_neighbors': 3, 'clf__weights': 'distance'}\n",
            "Best score: 0.2169811320754717\n",
            "\n",
            "\n",
            "Grid search duration: 1.107474959000001 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: word2vec\n",
            "Best hyperparameters: {'clf__metric': 'euclidean', 'clf__n_neighbors': 7, 'clf__weights': 'uniform'}\n",
            "Best score: 0.1761006289308176\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training for naive Bayes:"
      ],
      "metadata": {
        "id": "i08KlN7DY3gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run(df_dict, df['target'], keys, train_per=0.9, clf='naive_bayes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jhfPU84FE3b",
        "outputId": "194b4765-f566-4918-9752-605a3ca0464f"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid search duration: 0.29030486199997085 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: ohe\n",
            "Best hyperparameters: {'clf__alpha': 1.0}\n",
            "Best score: 0.24842767295597484\n",
            "\n",
            "\n",
            "Grid search duration: 0.3510722580000447 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: bow\n",
            "Best hyperparameters: {'clf__alpha': 1.0}\n",
            "Best score: 0.24213836477987422\n",
            "\n",
            "\n",
            "Grid search duration: 0.33870970500004205 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: tf\n",
            "Best hyperparameters: {'clf__alpha': 0.1}\n",
            "Best score: 0.2389937106918239\n",
            "\n",
            "\n",
            "Grid search duration: 0.3227986189999683 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: tf-idf\n",
            "Best hyperparameters: {'clf__alpha': 1.0}\n",
            "Best score: 0.22327044025157233\n",
            "\n",
            "\n",
            "Grid search duration: 0.3440397370000028 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: word2vec\n",
            "Best hyperparameters: {'clf__alpha': 0.001}\n",
            "Best score: 0.14779874213836477\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training for decision tree:"
      ],
      "metadata": {
        "id": "nmCaFFL0Y6H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run(df_dict, df['target'], keys, train_per=0.9, clf='decision_tree')"
      ],
      "metadata": {
        "id": "m-B2qmjgFEwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d96ae57-db40-41d5-d5f9-927538ca136a"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid search duration: 1.0542255029999978 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: ohe\n",
            "Best hyperparameters: {'clf__criterion': 'gini', 'clf__max_depth': None}\n",
            "Best score: 0.27358490566037735\n",
            "\n",
            "\n",
            "Grid search duration: 0.6858728889999384 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: bow\n",
            "Best hyperparameters: {'clf__criterion': 'entropy', 'clf__max_depth': 15}\n",
            "Best score: 0.25157232704402516\n",
            "\n",
            "\n",
            "Grid search duration: 0.7721577710000247 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: tf\n",
            "Best hyperparameters: {'clf__criterion': 'entropy', 'clf__max_depth': 15}\n",
            "Best score: 0.22641509433962265\n",
            "\n",
            "\n",
            "Grid search duration: 0.6949863600000299 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: tf-idf\n",
            "Best hyperparameters: {'clf__criterion': 'gini', 'clf__max_depth': 15}\n",
            "Best score: 0.23270440251572327\n",
            "\n",
            "\n",
            "Grid search duration: 4.917319666000026 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: word2vec\n",
            "Best hyperparameters: {'clf__criterion': 'gini', 'clf__max_depth': 5}\n",
            "Best score: 0.16037735849056603\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training for random forest:"
      ],
      "metadata": {
        "id": "5iQRZCOjY9At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run(df_dict, df['target'], keys, train_per=0.9, clf='random_forest')"
      ],
      "metadata": {
        "id": "IRQtdgk0FEqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d877949-8385-4d28-f2e4-df9c463324d2"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid search duration: 4.5750468610000325 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: ohe\n",
            "Best hyperparameters: {'clf__criterion': 'gini', 'clf__max_depth': None, 'clf__n_estimators': 50}\n",
            "Best score: 0.27672955974842767\n",
            "\n",
            "\n",
            "Grid search duration: 12.26889772000004 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: bow\n",
            "Best hyperparameters: {'clf__criterion': 'gini', 'clf__max_depth': None, 'clf__n_estimators': 300}\n",
            "Best score: 0.2893081761006289\n",
            "\n",
            "\n",
            "Grid search duration: 12.64024860699999 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: tf\n",
            "Best hyperparameters: {'clf__criterion': 'entropy', 'clf__max_depth': None, 'clf__n_estimators': 300}\n",
            "Best score: 0.3113207547169811\n",
            "\n",
            "\n",
            "Grid search duration: 12.95100331599997 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: tf-idf\n",
            "Best hyperparameters: {'clf__criterion': 'entropy', 'clf__max_depth': None, 'clf__n_estimators': 300}\n",
            "Best score: 0.29559748427672955\n",
            "\n",
            "\n",
            "Grid search duration: 46.36864012900003 seconds\n",
            "Model metrics:\n",
            "==============================\n",
            "Vectorizer: word2vec\n",
            "Best hyperparameters: {'clf__criterion': 'gini', 'clf__max_depth': 15, 'clf__n_estimators': 300}\n",
            "Best score: 0.1761006289308176\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observing the score results for each vectorizer in each ML model, it is clearly to see that there is no right order for the best stituation. Despite presenting low score results overall, the dataset has few peculiarities. Perhaps the data scraping should be better, or better data cleaning, etc.\n",
        "\n",
        "Looking for the actual results, the Word2Vec vectorizer had the lowest results for this dataset. Perhaps using the mean value of vector for each word in sentence was not good enough. In other way, the One-Hot-Encoder was the more adequate for the dataset of choice in most of the cases. For the random forest model the best vectorizer was the TF.\n",
        "\n",
        "Further imporvements may come with some research and experimentation, but the present work displayed a design flow methodology from gathering, cleaning and labeling the data, also performing feature extraction with multiples vectorizer and using multiples machine learning models to train."
      ],
      "metadata": {
        "id": "-qH3jj1Xc5S0"
      }
    }
  ]
}